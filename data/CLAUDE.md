# æ¨¡å—å¯¼èˆªï¼šdata/

> [æ ¹ç›®å½• CLAUDE.md](../CLAUDE.md)

---

## ğŸ“¦ æ¨¡å—æ¦‚è¿°

`data/` ç›®å½•åŒ…å«æ•°æ®é›†å¤„ç†å’Œå¾®è°ƒç›¸å…³çš„æ•°æ®åŠ è½½å™¨ã€æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†å·¥å…·ã€‚

---

## ğŸ“Š æ•°æ®é›†æ”¯æŒ

### æ”¯æŒçš„æ•°æ®é›†

| æ•°æ®é›† | ç”¨é€” | è¯´è¯äººæ•°é‡ | è¯­è¨€ | é‡‡æ ·ç‡ |
|--------|------|------------|------|--------|
| LibriTTS | è®­ç»ƒ/è¯„ä¼° | 1k+ | è‹±è¯­ | 24kHz |
| M4Singer | æ­Œå”±è¯„ä¼° | 4 | ä¸­æ–‡ | 44.1kHz |
| è‡ªå®šä¹‰æ•°æ®é›† | å¾®è°ƒ | å¯é…ç½® | å¤šè¯­è¨€ | å¯é…ç½® |

### æ•°æ®æ ¼å¼

**æ”¯æŒçš„éŸ³é¢‘æ ¼å¼**ï¼š
- WAV (æ¨è)
- FLAC
- MP3
- OGG

**æ•°æ®ç»„ç»‡**ï¼š
```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ speaker1/
â”‚   â”‚   â”œâ”€â”€ utt1.wav
â”‚   â”‚   â”œâ”€â”€ utt2.wav
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ speaker2/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ val/
â””â”€â”€ test/
```

---

## ğŸ”§ æ•°æ®å¤„ç†å·¥å…·

### [ft_dataset.py](ft_dataset.py) - å¾®è°ƒæ•°æ®é›†

**åŠŸèƒ½**ï¼šä¸ºæ¨¡å‹å¾®è°ƒæä¾›æ•°æ®åŠ è½½å’Œå¤„ç†èƒ½åŠ›

**ä¸»è¦ç‰¹æ€§**ï¼š
- å¤šè¯´è¯äººæ•°æ®åŠ è½½
- è¯´è¯äººå‡è¡¡é‡‡æ ·
- éŸ³é¢‘é¢„å¤„ç†
- æ•°æ®å¢å¼º
- æ‰¹å¤„ç†æ”¯æŒ

**å…³é”®ç±»**ï¼š
- `FineTuningDataset`: å¾®è°ƒæ•°æ®é›†ç±»
- `DataLoader`: æ•°æ®åŠ è½½å™¨
- `Collator`: æ•°æ®æ•´ç†å™¨

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from data.ft_dataset import FineTuningDataset

# åˆ›å»ºæ•°æ®é›†
dataset = FineTuningDataset(
    data_dir="data/train",
    sample_rate=24000,
    segment_length=48000,
    hop_length=256,
    speaker2id_path="speaker2id.json"
)

# åŠ è½½æ•°æ®
dataloader = torch.utils.data.DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    pin_memory=True
)
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| data_dir | str | å¿…éœ€ | æ•°æ®ç›®å½•è·¯å¾„ |
| sample_rate | int | 24000 | ç›®æ ‡é‡‡æ ·ç‡ |
| segment_length | int | 48000 | éŸ³é¢‘æ®µé•¿åº¦ |
| hop_length | int | 256 | è·³è·ƒé•¿åº¦ |
| speaker2id_path | str | None | è¯´è¯äººIDæ˜ å°„ |
| use_aug | bool | False | æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º |

---

## ğŸ“‹ æ•°æ®é¢„å¤„ç†æµç¨‹

### 1. éŸ³é¢‘åŠ è½½

```python
def load_audio(file_path, target_sr=24000):
    """åŠ è½½éŸ³é¢‘æ–‡ä»¶å¹¶è½¬æ¢é‡‡æ ·ç‡"""
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(file_path, sr=None)

    # è½¬æ¢é‡‡æ ·ç‡
    if sr != target_sr:
        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)

    # å½’ä¸€åŒ–
    audio = audio / np.max(np.abs(audio))

    return audio
```

### 2. éŸ³é¢‘åˆ†æ®µ

```python
def segment_audio(audio, segment_length=48000):
    """å°†éŸ³é¢‘åˆ†å‰²ä¸ºå›ºå®šé•¿åº¦æ®µ"""
    if len(audio) < segment_length:
        # å¡«å……
        padding = segment_length - len(audio)
        audio = np.pad(audio, (0, padding), mode='constant')
    else:
        # éšæœºæˆªå–
        start = np.random.randint(0, len(audio) - segment_length + 1)
        audio = audio[start:start + segment_length]

    return audio
```

### 3. ç‰¹å¾æå–

```python
def extract_features(audio, sr=24000):
    """æå–éŸ³é¢‘ç‰¹å¾"""
    # æå– F0
    f0 = extract_f0(audio, sr)

    # æå–æ¢…å°”é¢‘è°±
    mel = librosa.feature.melspectrogram(
        y=audio,
        sr=sr,
        n_mels=80,
        hop_length=256,
        win_length=1024
    )

    # è½¬æ¢ä¸ºå¯¹æ•°æ¢…å°”é¢‘è°±
    mel = librosa.power_to_db(mel)

    return {
        'audio': audio,
        'f0': f0,
        'mel': mel
    }
```

---

## ğŸ­ æ•°æ®å¢å¼º

### æ”¯æŒçš„æ•°æ®å¢å¼º

| å¢å¼ºç±»å‹ | æ–¹æ³• | å‚æ•° | æ•ˆæœ |
|----------|------|------|------|
| éŸ³é‡å¢å¼º | éšæœºå¢ç›Š | gain_range=(0.7, 1.3) | æé«˜é²æ£’æ€§ |
| å™ªå£°å¢å¼º | æ·»åŠ å™ªå£° | snr_range=(20, 40) dB | æŠ—å™ªè®­ç»ƒ |
| æ—¶ç§»å¢å¼º | æ—¶é—´å¹³ç§» | shift_range=0.1 | ä½ç½®ä¸å˜æ€§ |
| éŸ³é«˜å¢å¼º | éŸ³é«˜åç§» | pitch_range=Â±2 semitone | F0 é²æ£’æ€§ |
| è¯­é€Ÿå¢å¼º | è¯­é€Ÿå˜åŒ– | speed_range=(0.9, 1.1) | è¯­é€Ÿé€‚åº” |

### æ•°æ®å¢å¼ºç¤ºä¾‹

```python
from data.augmentation import TimeShift, PitchShift, SpeedChange

# å®šä¹‰å¢å¼ºç®¡é“
augmentations = [
    TimeShift(shift_prob=0.3, max_shift=0.1),
    PitchShift(pitch_prob=0.2, max_shift=2),
    SpeedChange(speed_prob=0.2, speed_range=(0.9, 1.1)),
    VolumeJitter(vol_prob=0.3, gain_range=(0.7, 1.3)),
    AddNoise(noise_prob=0.1, snr_range=(20, 40))
]

# åº”ç”¨å¢å¼º
def apply_augmentation(audio, features):
    for aug in augmentations:
        audio, features = aug(audio, features)
    return audio, features
```

---

## ğŸ”„ æ•°æ®å¹³è¡¡

### è¯´è¯äººå‡è¡¡

```python
def balance_speakers(dataset, min_utterances=100):
    """å‡è¡¡è¯´è¯äººæ•°æ®åˆ†å¸ƒ"""
    speaker_counts = {}
    for idx in range(len(dataset)):
        speaker = dataset.speakers[idx]
        speaker_counts[speaker] = speaker_counts.get(speaker, 0) + 1

    # è¯†åˆ«æ•°æ®ä¸è¶³çš„è¯´è¯äºº
    under_represented = [
        spk for spk, count in speaker_counts.items()
        if count < min_utterances
    ]

    # å¯¹ä¸è¶³çš„è¯´è¯äººè¿›è¡Œè¿‡é‡‡æ ·
    balanced_indices = []
    for spk in under_represented:
        indices = dataset.get_speaker_indices(spk)
        balanced_indices.extend(indices * (min_utterances // len(indices) + 1))

    return balanced_indices
```

---

## ğŸ“ æ•°æ®é›†åˆ›å»º

### åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†

```python
import json
from pathlib import Path

def create_dataset_metadata(data_dir, output_path):
    """åˆ›å»ºæ•°æ®é›†å…ƒæ•°æ®"""
    metadata = {
        "sample_rate": 24000,
        "channels": 1,
        "format": "wav",
        "speakers": {}
    }

    # æ‰«ææ•°æ®ç›®å½•
    for speaker_dir in Path(data_dir).iterdir():
        if speaker_dir.is_dir():
            speaker_id = speaker_dir.name
            audio_files = list(speaker_dir.glob("*.wav"))

            metadata["speakers"][speaker_id] = {
                "utterances": len(audio_files),
                "files": [str(f) for f in audio_files],
                "total_duration": sum(get_duration(f) for f in audio_files)
            }

    # ä¿å­˜å…ƒæ•°æ®
    with open(output_path, 'w') as f:
        json.dump(metadata, f, indent=2)

# ä½¿ç”¨
create_dataset_metadata("data/train", "data/metadata.json")
```

---

## ğŸ“Š æ•°æ®ç»Ÿè®¡

### æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯

```python
def dataset_statistics(dataset):
    """è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        "num_speakers": len(dataset.speakers),
        "num_utterances": len(dataset),
        "total_duration": 0,
        "avg_duration": 0,
        "min_duration": float('inf'),
        "max_duration": 0,
        "sampling_rate": dataset.sample_rate
    }

    for idx in range(len(dataset)):
        audio = dataset[idx]['audio']
        duration = len(audio) / dataset.sample_rate

        stats["total_duration"] += duration
        stats["min_duration"] = min(stats["min_duration"], duration)
        stats["max_duration"] = max(stats["max_duration"], duration)

    stats["avg_duration"] = stats["total_duration"] / len(dataset)

    return stats

# æ‰“å°ç»Ÿè®¡ä¿¡æ¯
stats = dataset_statistics(dataset)
print(f"è¯´è¯äººæ•°é‡: {stats['num_speakers']}")
print(f"è¯­éŸ³æ®µæ•°: {stats['num_utterances']}")
print(f"æ€»æ—¶é•¿: {stats['total_duration']:.2f} ç§’")
print(f"å¹³å‡æ—¶é•¿: {stats['avg_duration']:.2f} ç§’")
```

---

## ğŸ¯ å¾®è°ƒæœ€ä½³å®è·µ

### æ•°æ®å‡†å¤‡æŒ‡å—

1. **æ•°æ®è´¨é‡**
   - ä½¿ç”¨æ¸…æ™°æ— å™ªçš„è¯­éŸ³
   - ç¡®ä¿é‡‡æ ·ç‡ä¸€è‡´
   - ç§»é™¤è¿‡çŸ­(<1ç§’)æˆ–è¿‡é•¿(>30ç§’)çš„éŸ³é¢‘

2. **è¯´è¯äººå¹³è¡¡**
   - æ¯ä¸ªè¯´è¯äººè‡³å°‘ 100 æ®µè¯­éŸ³
   - ä¿æŒæ—¶é•¿åˆ†å¸ƒå‡åŒ€
   - åŒ…å«ä¸åŒæƒ…æ„Ÿå’Œè¯­è°ƒ

3. **æ•°æ®å¢å¼º**
   - é€‚åº¦ä½¿ç”¨å¢å¼ºï¼ˆä¸è¶…è¿‡ 30%ï¼‰
   - é’ˆå¯¹ç‰¹å®šåœºæ™¯å®šåˆ¶å¢å¼ºç­–ç•¥
   - è®°å½•å¢å¼ºå‚æ•°

4. **éªŒè¯é›†åˆ’åˆ†**
   - 80% è®­ç»ƒ / 10% éªŒè¯ / 10% æµ‹è¯•
   - ä¿æŒè¯´è¯äººåˆ†å¸ƒä¸€è‡´
   - ä½¿ç”¨çœŸå®è¯­éŸ³ä½œä¸ºéªŒè¯é›†

### å¾®è°ƒé…ç½®ç¤ºä¾‹

```yaml
# config/finetune.yaml
data:
  train_dir: "data/train"
  val_dir: "data/val"
  batch_size: 32
  segment_length: 48000
  sample_rate: 24000
  num_workers: 4

  augmentation:
    time_shift: true
    pitch_shift: true
    volume_jitter: true
    add_noise: false

model:
  pretrained_path: "checkpoints/pretrained_v2.pt"
  freeze_encoder: false
  learning_rate: 1e-4
  weight_decay: 1e-5

training:
  epochs: 100
  save_interval: 10
  eval_interval: 5
  gradient_clip: 1.0
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### æ•°æ®åŠ è½½ä¼˜åŒ–

```python
# ä½¿ç”¨é¢„å–ç¼“å†²åŒº
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=4,
    pin_memory=True,
    persistent_workers=True,
    prefetch_factor=2
)

# ç¼“å­˜ç‰¹å¾
dataset.cache_features = True
dataset.cache_dir = "data/cache"

# å†…å­˜æ˜ å°„
dataset.use_memory_map = True
dataset.map_file = "data/audios.dat"
```

---

*æ­¤æ–‡æ¡£ç”± Claude Code è‡ªåŠ¨ç”Ÿæˆäº 2025-10-28*
